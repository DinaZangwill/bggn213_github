---
title: "Untitled"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

```{r}
### use kmeans() setting k to 2 nstart to 20 to answer some questions about the above graph
#Use the kmeans() function setting k to 2 and nstart=20
#Inspect/print the results
#Q. How many points are in each cluster?
#Q. What ‘component’ of your result object details
 #- cluster size?
 #- cluster assignment/membership?
 #- cluster center?
#Plot x colored by the kmeans cluster assignment and
 #add cluster centers as blue points
k<-kmeans(x,centers=2,nstart=20)

```
```{r}
k
```
# you can see the availble components of which there are 9
#how to call up components is k$component 
```{r}
#Q. How many points are in each cluster?
k$size
#this returns [1] 30 30. this means 30 points in each cluster 
#Q. What ‘component’ of your result object details
 #- cluster size?
 ###i think it's k$size? is this a trick? it's 30???
table(k$cluster) ## 30 1s and 30 2s
 #- cluster assignment/membership?

 #- cluster center?
```
# now let's:
```{r}
#Plot x colored by the kmeans cluster assignment and
 #add cluster centers as blue points
kmeans(x, centers=2, nstart=20)
plot(x, col=k$cluster) #col=k$cluster will color the clusters
points(k$centers, col="blue", pch=15) #we did pch=15 to really make the centers stand out
```


#Hierarchical clustering 
#we dont know the number of clusters ahead of time 
#can be bottom-up or top-down
#more flexible than k-means
```{r}
#we need the x again
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```
```{r}
##so first let's try this
hc<-hclust(dist(x))
hc ###ehhh that's not helpful but lets plot is
plot(hc) ##look at the numbers and how they are organized
#they are 30 and below (left) and above 30 (right) 
## draw a line on the tree using abline
abline(h=6, col="red")
abline(h=4, col="blue")
cutree(hc, k=6)
cutree(hc, k=4)

```
```{r}
## you can also cut trees to yield a given k groups/clusters
grps<-cutree(hc, k=2)
table(grps)
```

```{r}
# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)
##let's investigate this new thing
```


```{r}
# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)
```


```{r}
# The hclust() function returns a hierarchical
# clustering model
hc_2 <- hclust(d = dist_matrix)
# the print method is not so useful here
hc_2
#Call:
#hclust(d = dist_matrix)
#Cluster method : complete
#Distance : euclidean
#Number of objects: 60 
```

## how to we link clusters in hierclusters?
##aka: how is distance between clusters determined? FOUR MAIN METHODS
### complete
#### pairwise similarity between all observations in clusters 1 and 2, uses largest of similarties
### single
#### uses smallest of similarities 
### average
#### uses average of similarities
### centroid 
#### finds the centroid for each cluster and uses similarity between those
### so how to do this in R
```{r}
# Using different hierarchical clustering methods
#hc.complete <- hclust(d, method="complete")
#hc.average <- hclust(d, method="average")
#hc.single <- hclust(d, method="single")
### d doesn't exist so lol this does shit right now it's just a format
```
### MY TURN!!! 
```{r}
# Step 1. Generate some example data for clustering
m <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(m) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(m)
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(m, col=col)
```

```{r}
#Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters
#Q. How does this compare to your known 'col' groups?
dist(m)
hc_myturn<-hclust(dist(m))
plot(hc_myturn)
abline(h=2.75, col="red")
grps_m2<- cutree(hc_myturn, k=2) ## for two groups
plot(m, col=grps_m2)
###now three groups
grps_m3<- cutree(hc_myturn, k=3)
plot(m, col=grps_m3)
```
### Principal Component Analysis (PCA)
## what is PCA???
## prcomp()
#we will draw and generate PCA plots, determine how much variation each principal component accounts for 
# examine loadings (or loading scores) to determine what variables have the largest effect on the graph
## ok lets's go
## You can also download this file from the class website!

```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV", row.names=1) #samples are columns, rows are genes
### using nrow(mydata) and ncol(mydata) you can see there are 10 cols and 100 rows
#time to implement PCA
#prcomp() expectssamples to be rows and genes to be columns (why????), so we gotta transpose the matrix with t()
mydata_t<-t(mydata)
#nice!
pca<-prcomp(mydata_t, scale = TRUE)
### check it out
attributes(pca)
###
```
```{r}
### let's look closer
pca$x[,1]
pca$x[,2]
#plot these??
plot(pca$x[,1], pca$x[,2]) ### see the larger variance with 1 compared to 2
```
#
```{r}
#ok we gotta look at out PCAs
summary(pca) #notice how PC1 has 92% of the proportion of variance
```

#
```{r}
## lets do PCA
pca <- prcomp(mydata_t, scale=TRUE)
## A basic PC1 vs PC2 2-D plot
plot(pca$x[,1], pca$x[,2])
## Variance captured per PC
pca.var <- pca$sdev^2 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1) 
pca.var.per
plot(pca)
```
```{r}
#let's colors the KO vs WT
plot(pca$x[,1:2], col=c("red","red","red","red","red", "blue","blue","blue","blue","blue")) #but this assumes i know that all the WT data comes before the KO so i can just list the color for each point
### that's really inefficient and ridiculous
```


```{r}
wt_ko<- substr(colnames(mydata),1,2) #idk what this does???
wt_ko
###you can make the plot beter with this i guess?
colvec<- as.factor(wt_ko) ### idk how this specified color
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
 xlab=paste0("PC1 (", pca.var.per[1], "%)"),
 ylab=paste0("PC2 (", pca.var.per[2], "%)")) 

```
#MY TURN AGAIN load the FILE
```{r}
#x<-read.csv("UK_foods.csv")
#summary(x)
#dim(x)
```

#preview the first six rows
```{r}
#head(x)
```
#let's  index it 
```{r}
#rownames(x) <- x[,1]
#x <- x[,-1]
#head(x)
#i dont know what we did this makes no sense
```


###now let's index it from the start bc using that -1 is ANNOYING AND CAUSES PROBLEMS
```{r}
#x <- read.csv("UK_foods.csv", row.names=1)
#head(x)
#i dont get this either
```
#barblot it
```{r}
#barplot(as.matrix(x), beside=T, col=rainbow(nrow(x))) #idk how this happened
```
#try a different plot

### and another way
```{r}
#pairs(x, col=rainbow(10), pch=16)
```

